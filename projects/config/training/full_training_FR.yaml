total_epochs: 150
num_gpus: 3
# Network option iresentXX, resnetXX, mobilenet_v2, mobilenet_v3_large, mobilenet_v3_small
backbone: iresnet50

load_weights:
  backbone:
    load_backbone: False
    # from https://onedrive.live.com/?authkey=%21AFZjr283nwZHqbA&id=4A83B6B633B029CC%215577&cid=4A83B6B633B029CC
    local_weights: False
    local_path: False
    freeze_layers: None
    last_layer_name: fc
    name: backbone.pth
  classifier:
    load_classifier: False
    local_weights: False
    local_path: None
    freeze_layers: False
    last_layer_name: False
    name: rank_0_softmax_weight.pt

early_stopping:
  enabled: True
  patience: 15
  min_epoch: 20 # Epoch from which the early stopping is used

batchSizeScheduler:
  enabled: False # we do not use it for now to simplify the training process and avoid possible bugs.
  patience: 10 # If you want to use it together with early stopping this value
  # must be smaller than the one used in the early stopping
  min_epoch: 50 # Epoch from which the batch size scheduler is used
  final_batch_size: 8

data_loaders:
  train:
    batch_size: 64
    num_workers: 10
    shuffle: True
    split: train
  val:
    batch_size: 64
    num_workers: 10
    shuffle: False
    split: val
  test_same_ID:
    batch_size: 64
    num_workers: 10
    shuffle: False
    split: test_same_ID
  test_FAR:
    batch_size: 64
    num_workers: 10
    shuffle: False
    split: test
optimizer:
  _target_: torch.optim.SGD
  lr: 0.00001
  momentum: 0.9
  weight_decay: 5e-4
lr_scheduler:
  _target_: projects.utils.utils.PolyScheduler
  base_lr: 0.1
  warmup_epoch: 5
  last_epoch: -1

focal_loss:
  _target_: src.losses.focal_loss.FocalLoss
  gamma: 2.0
  alpha: 0.25
entropy_loss:
  _target_: torch.nn.CrossEntropyLoss
